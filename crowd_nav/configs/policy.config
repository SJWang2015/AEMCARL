# policy configurations for robot

[rl]
gamma = 0.9


[om]
cell_num = 4
cell_size = 1
om_channel_size = 3


[action_space]
kinematics = holonomic
# action space size is speed_samples * rotation_samples + 1
speed_samples = 5
rotation_samples = 16
sampling = exponential
query_env = true


[cadrl]
mlp_dims = 150, 100, 100, 1
multiagent_training = false


[lstm_rl]
global_state_dim = 50
mlp1_dims = 150, 100, 100, 50
mlp2_dims = 150, 100, 100, 1
multiagent_training = true
with_om = false
with_interaction_module = false


[srl]
mlp1_dims = 150, 100, 100, 50
mlp2_dims = 150, 100, 100, 1
multiagent_training = true
with_om = false

[sarl]
mlp1_dims = 150, 100
mlp2_dims = 100, 50
attention_dims = 100, 100, 1
mlp3_dims = 150, 100, 100, 1
multiagent_training = true
with_om = false
with_global_state = true

[comcarl]
in_mlp_dims = 100, 50
sort_mlp_dims = 100, 50
sort_attention_dims = 100, 100, 1
action_dims = 150, 100, 100, 1
global_state_dim = 50
dropout = 0.5
alpha = 1.0
nheads = 7
multiagent_training = true
with_om = false
with_global_state = true


[gipcarl]
in_mlp_dims = 64, 64, 32
ia_mlp_dims = 32, 32
sort_mlp_dims = 64, 32
global_state_dim = 32
sort_attention_dims = 64, 64, 1
aggregation_dims = 64, 64
action_dims = 128, 64, 32, 1
with_interaction = true
with_om = false
with_global_state = true
multiagent_training = true


[actcarl]
in_mlp_dims = 100, 50
action_dims = 100, 50, 50, 1
with_dynamic_net = true
multiagent_training = true
with_global_state = false
with_om = false


[actenvcarl]
in_mlp_dims = 100, 50
sort_mlp_dims = 100, 50
sort_attention_dims = 50, 50, 1
action_dims = 100, 50, 1
global_state_dim = 50
with_dynamic_net = true
multiagent_training = true
with_global_state = false
with_om = false